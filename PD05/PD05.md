# PD05
* Mauricio Bernuy
* Claudia Noche
* Anthony Aguilar

[ToC]

## Ejercicio 1: Adivina mi número con MPI

#### Genere un modelo en MPI que simule la siguiente situación:
#### Eduardo está con 4 amigos. El piensa un número entre 1 y 100, y dice a sus amigos: **"¡Adivinen mi número!"**. Sus amigos solo saben que está entre 1 y 100, deben escribir el número en un papel y mostrarlo simultáneamente. Si logran todos adivinar el número en el mismo intento, Eduardo los invita a cenar al Restaurante Central (el más caro de Lima). Eduardo está de buen humor, pero además confía en la teoria estadística, y les da **1000 intentos** para hacerlo.

### Item a)
#### Modelo:

<details>
  <summary>Expandir Código</summary>
    
```cpp
#include <mpi.h>
#include <iostream>
#include <math.h>
#include <time.h>
#include <stdio.h>
#include <stdlib.h>
#include <random>

using namespace std;

#define N 1000
#define s 100

int main(int argc, char **argv)
{
  std::random_device rd;
  std::mt19937 mt(rd());
  std::uniform_int_distribution<int> dist(1, s);

  int size, rank;

  MPI_Init(&argc, &argv);
  MPI_Comm_size(MPI_COMM_WORLD, &size);
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  int EduardoNumero, NumeroAmigo;
  int comps;

  if (rank == 0)
  {
    EduardoNumero = dist(mt);
    // comentar para tester
    printf("Numero de Eduardo: %d\n", EduardoNumero); 
  }

  // Broadcast
  MPI_Bcast(&EduardoNumero, 1, MPI_INT, 0, MPI_COMM_WORLD);

  bool found = false;
  for (int i = 0; i < N; i++)
  {
    NumeroAmigo = dist(mt);
    comps = NumeroAmigo == EduardoNumero;

    int compbuf = 0;
    // Reduce
    MPI_Reduce(&comps, &compbuf, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);   

    // check 
    if (rank == 0)
    {
      if (compbuf == 4)
      {
        found = true;
        break;
      }
    }
  }
  if (rank == 0)
  {
    if (found)
      printf("found\n");
    else
      printf("not found\n");
  }
  
  MPI_Finalize();
  return 0;
}

```
</details>

###### > p1.cpp


#### Según el modelo creado ¿Tendrá que invitar Eduardo a sus amigos? Fundamente la respuesta con los experimentos y compruebe si se acercan a la probabilidad de éxito según la teoría estadística.
### Item b)

Fórmula de probabilidad:
$$ p = {(\frac{1}{s}\cdot\frac{1}{s}\cdot\frac{1}{s}\cdot\frac{1}{s})\cdot N}\\ s = 100,\  N=1000 $$

***s*** definiendo el rango de números hasta el 100 y **n** el número de intentos. Se puede ver como un problema de elección con repetición, eligiendo 4 veces el mismo número y repitiendo el experimento N veces.
$$ p=\frac{1}{100000} = 0.00001 $$

Como vemos, las probabilidades de que los amigos encuentren el mismo número resultan ser extremadamente bajas, por lo que es casi seguro decir que **no tendrá que invitar a sus amigos.**

Con el fin de poder observar el comportamiento en las pruebas experimentales, se experimenta con un rango de numeros s del 1 al 10 y un número de intentos N igual a 100, lo cual resultaría una probabilidad de **0.01**. Se usa un script de python para correr el algoritmo **500 veces** y se outputea el porcentaje de selecciones correctas.


<details>
  <summary>Expandir Código</summary>
    
```python
from subprocess import Popen, PIPE

pr = 4
iterations = 500
program_path = f'mpirun -np {pr} ./a.out'

print("\nUsing Iterations:", iterations)
count_founds = 0
for i in range(iterations):
  p = Popen([program_path], shell=True, stdout=PIPE, stdin=PIPE)
  p.stdin.flush()
  result = p.stdout.readline().strip().decode()
  if result == "found":
    count_founds += 1
  print("iter:",i,"result:", result)
prob = count_founds / iterations
print(f'probability ={prob:.20f}')

```
</details>

###### > tester.py


Obtenemos entonces: 

| ![](https://i.imgur.com/NFu941S.png) | 
|:--:| 
| *p=0.01* |

El resultado experimental coincide con lo esperado del algoritmo usando esas constantes.

El experimento con las variables de s=100, N=1000 se podrían realizar utilizando un numero de 500000 iteraciones, pero esto tomaría una cantidad de tiempo excesiva debido al running time del algoritmo (aproximamos unos 5 días!).

---

## Ejercicio 2: BucketSort

    


#### Adapte la función del algoritmo bucket sort a un programa en paralelo usando memoria compartida (OMP). Se adjunta código secuencial (bucketsort.cpp)
    
Utilizando librería: **https://github.com/swenson/sort** para implementar los sorters por hilo.
    
##  Código paralelizado:
<details>
<summary>Expandir Código</summary>
    
```cpp
#ifdef PARALLEL_EXEC
#include <omp.h>
#endif
#include <math.h>
#include <time.h>
#include <vector>
#include <stdio.h>
#include <stdlib.h>
#include <iostream>
#include <algorithm>

#define SORT_NAME float
#define SORT_TYPE float
#define SORT_CMP(x, y) ((x) - (y))
#include "./sort.h"

void bucketSort(float *arr, int &n, int bucket_size)
{
  // Crear buckets
  std::vector<float> bucket[bucket_size];
#ifdef PARALLEL_EXEC
  omp_lock_t *lock = new omp_lock_t[bucket_size];
#pragma omp parallel for
  for (int k = 0; k < bucket_size; ++k)
    omp_init_lock(&lock[k]);
#endif
  int i, j, bucket_index, index = 0;

  // asignar elementos a los buckets
  for (i = 0; i < n; ++i)
  {
    bucket_index = bucket_size * arr[i] / 1000;
#ifdef PARALLEL_EXEC
    omp_set_lock(&lock[bucket_index]);
#endif
    bucket[bucket_index].push_back(arr[i]);
#ifdef PARALLEL_EXEC
    omp_unset_lock(&lock[bucket_index]);
#endif
  }

// ordenar buckets
#pragma omp parallel for
  for (j = 0; j < bucket_size; ++j)
  {
    // #ifdef PARALLEL_EXEC
    // printf("Bucket: %d in thread: %d\n", j, omp_get_thread_num());
    // #endif
#ifdef USE_SORTERS
    double start;
    double end;
    start = omp_get_wtime();
    // to use the lib
    float temp[bucket[j].size()];
    std::copy(bucket[j].begin(), bucket[j].end(), temp);
    switch (j)
    {
    case 0:
      float_quick_sort(temp, bucket[j].size());
      break;
    case 1:
      float_merge_sort(temp, bucket[j].size());
      break;
    case 2:
      float_heap_sort(temp, bucket[j].size());
      break;
    default:
      float_shell_sort(temp, bucket[j].size());
      break;
    }
    end = omp_get_wtime();
    printf("Bucket %d sorting took %f seconds\n", omp_get_thread_num(), end - start);
    bucket[j] = std::vector<float>(temp, temp + bucket[j].size());
#else
    sort(bucket[j].begin(), bucket[j].end());
#endif
  }
  // Concatenar buckets en arr[]
  for (i = 0; i < bucket_size; i++)
  {
    for (j = 0; j < bucket[i].size(); j++)
      arr[index++] = bucket[i][j];
  }
#ifdef PARALLEL_EXEC
#pragma omp parallel for
  for (int k = 0; k < bucket_size; ++k)
    omp_destroy_lock(&lock[k]);
  delete[] lock;
#endif
}

int main(int argc, char *argv[])
{
  int THREADS = 1;
  std::cin >> THREADS;
#ifdef PARALLEL_EXEC
  omp_set_num_threads(THREADS);
#endif
  int i, n = pow(2, 19);
  float *randArray;

  srand((int)time(0));

  randArray = new float[n];

  for (int i = 0; i < n; ++i)
    randArray[i] = (float)rand() / (float)(RAND_MAX / 999.0);

  // for (i = 0; i < n; ++i)
  //   printf("%1.2f, ", randArray[i]);
  // printf("\n");

  // ordenar array en buckets
  double s = omp_get_wtime();
  bucketSort(randArray, n, THREADS);
  double e = omp_get_wtime();
  printf("BucketSort sorting took %f seconds\n", e - s);

  // for (i = 0; i < n; ++i)
  //   printf("%1.2f, ", randArray[i]);

  delete[] randArray;
}
```
                       
```
flags:
-DPARALLEL_EXEC - ejecutar el codigo en paralelo
-DUSE_SORTERS - usa algoritmos de sorting en vez de std::sort
```

</details>
    
###### > p2.cpp
    
### Item a)
#### Utilice una lista de 2^19 floats generados aleatoriamente y mida los tiempos de ejecución en 1, 2, 4, 8 , ... procesos.

Usando el siguiente script de en python:
    
<details>
<summary>Expandir Código</summary>
    
```python
    
from subprocess import Popen, PIPE

pr = 8
iterations = 100
program_path = "./a.out"

print("   processors", "  |\t","average(s)")
print("-------------------------------------")

for pr_i in range(1,pr+1):
  avg = 0
  for i in range(iterations):
    p = Popen([program_path], shell=True, stdout=PIPE, stdin=PIPE)
    p.stdin.write((str(pr_i)+"\n").encode())
    p.stdin.flush()
    result = p.stdout.readline().strip().decode()
    avg += float(result)
  avg = avg / iterations
  print("\t",pr_i, "\t|  ",f'{avg:.20f}')

```

</details>

###### > tester.py


Se obtiene la siguiente tabla: 
    
| ![](https://i.imgur.com/LSndWqa.png) | 
|:--:| 
| *Output de consola, promedios de 100 iteraciones* |

Se puede apreciar como el algoritmo mejora favorablemente conforme aumenta el numero de threads.

### Item b)
#### Decida que método de ordenamiento a usar en cada hilo y determine los tiempos promedio de ejecución de ordenamiento local en cada hilo

Usando una librería hemos utilizado el siguiente esquema: 

    
| Thread | Algoritmo | 
| -------- | -------- |
|   0   | quicksort     |
|   1   | mergesort     |
|   2   | heapsort     |
| **after**     | shellsort     |
    
El código se ejecuta con la flag -DUSE_sorters para utilizar estos algoritmos.
    
Utilizando nuevamente un script en python:
    
        
<details>
<summary>Expandir Código</summary>
    
```python
    
from subprocess import Popen, PIPE

pr = 4
iterations = 100
program_path = "./a.out"

print("   processors", "  |\t","average(s)")
print("-------------------------------------")

avg = 0
buckets = [0] * pr
for i in range(iterations):
  p = Popen([program_path], shell=True, stdout=PIPE, stdin=PIPE)
  p.stdin.write((str(pr)+"\n").encode())
  p.stdin.flush()
  result = p.stdout.readline().strip().decode()
  res_split = result.split(",")
  res_split.pop()
  for x in res_split:
    cur = x.split(":")
    buckets[int(cur[0])] += float(cur[1])

for i in range(len(buckets)):
  buckets[i] = buckets[i] / iterations 
  print("\t",i, "\t|  ",f'{buckets[i]:.20f}')

```

</details>

###### > tester2.py

Obtenemos los siguientes resultados:

| ![](https://i.imgur.com/Wlg66w4.png) | 
|:--:| 
| *Figura A, Output de consola, promedios de 100 iteraciones* |

Como es de esperarse, quicksort lidera con el menor tiempo promedio de ejecución.
    
### Item c)
    
#### Determine el tiempo de cómputo de forma teórica y experimental. ¿Cómo aproximaria el tiempo de acceso a memoria en forma teórica? ¿Lo puede observar en el experimento? Compare los resultados en una gráfica. Asimismo, compruebe la validez experimental-teórica de los tiempos locales.

| Algoritmo | Complejidad teórica promedio |
| --------- | ----------- |
|   Quicksort  | $$ O(n log(n)) $$  |
|   Mergesort  | $$ O(n log(n)) $$  |
|   Heapsort   | $$ O(n log(n)) $$  |
|   Shellsort  | $$ O(n log(n)) $$  |
    
La tabla explica la complejidad computacional promedio y complejidad espacial de cada algoritmo. En la Figura A, la imágen de la sección previa, se puede ver el tiempo de ejecución de cada algoritmo en el código.

La discrepancia en los tiempos de ejecución reales y teóricos se debe a que, como se explicó anteriormente, el tiempo de ejecución presenta una fórmula para el valor promedio. En las iteraciones, pueden salir tiempos de ejecución más lentos o rápidos, dependiendo de los valores reales. Por otro lado, se tienen posibles problemas de optimización en la implementación de los códigos de cada algoritmo. 
    
Sin embargo, cabe mencionar que, aparte de Quicksort, que corre mucho más rápido que los otros algoritmos, los valores no se alejan mucho entre ellos, todos teniendo un error experimental de 0.0158 segundos entre ellos. 
    
|![](https://i.imgur.com/7SHFABH.png)| 
|:--:| 
| *Figura B, Comparación de tiempos de ejecución reales (Azul) vs teóricos (Naranja)* |

  
    
---

## Ejercicio 3: : Multiplicación matriz-vector con MPI+OMP
Considere el código de multiplicación matriz-vector de la PD3 (Ej.3).

### Item a)
#### Muestre una gráfica tiempo de ejecución vs. numero de procesos en MPI, para np=2,4,8,16
 ![](https://i.imgur.com/XOjKJwl.png)
    
![](https://i.imgur.com/UcEEOGp.png)
    

### Item b)
#### Paralelice la multiplicación matriz-vector con OMP, y genere una gráfica tiempo de ejecución vs. número de threads, para nthreads=2,4,8,16

![](https://i.imgur.com/Fk7fXc7.png)
![](https://i.imgur.com/I23sEvP.png)

### Item c) 
#### Paralelice la multiplicación matriz-vector con MPI+OMP, y compare en una gráfica velocidad vs. número de procesos, las curvas para np=2,4,8, . . . y nthreads = 2,4,6,... con cada caso en MPI  
    ![](https://i.imgur.com/fQLHHVL.png)

### Item d) 
#### Discuta la escalabilidad de los resultados y decida sobre la utilidad de implementar un modelo híbrido para este problema

Un modelo híbrido para el problema de multiplicacion de matrices puede lograr beneficiar de manera, pues logramos utilizar el concepto de paralelismo distribuido de MPI mandando data a diferentes computadores en el  cluster, y luego aprovechando el paralelismo compartido usando los recursos (threads) de cada computadora de manera independiente, de esta manera aprovechando al máximo los recursos disponibles.
    
    
#### Compile y ejecute este análisis en el cluster Khipu. Incluya los scripts usados y describa los parámetros asignados para procesos MPI e hilos OMP

Los scripts .sh y todo el código se encuentra en la carpeta p3, y se siguió la guía de Khipu para enviar los tasks. Lamentablemente, debido al las largas queues no se lograron tener los resultados para esta entrega.